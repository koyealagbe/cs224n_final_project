{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rb1BSLugcZn",
        "outputId": "31f7bb6d-bfd0-4805-8e61-ab46ab32bbd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 28.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
          ]
        }
      ],
      "source": [
        "# First upload the training and evaluation files to this runtime (Press connect if needed)\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ltEwIIMDg9ex"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    AutoConfig,\n",
        "    GPT2LMHeadModel,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    LineByLineTextDataset,\n",
        "    PreTrainedTokenizer,\n",
        "    TextDataset,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get access to model types and model configs to select GPT2 model and config\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IL0YR9WMhG0v"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\"\n",
        "        },\n",
        "    )\n",
        "    model_type: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"If training from scratch, pass a model type from the list: \"\n",
        "            + \", \".join(MODEL_TYPES)\n",
        "        },\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Where do you want to store the pretrained models downloaded from s3\"\n",
        "        },\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fX9aWWf1hL4X"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    train_data_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a text file).\"}\n",
        "    )\n",
        "    eval_data_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"\n",
        "        },\n",
        "    )\n",
        "    line_by_line: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    mlm: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": \"Train with masked-language modeling loss instead of language modeling.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    block_size: int = field(\n",
        "        default=-1,\n",
        "        metadata={\n",
        "            \"help\": \"Optional input sequence length after tokenization.\"\n",
        "            \"The training dataset will be truncated in block of this size for training.\"\n",
        "            \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "styWzKnTmps1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class BosToEosTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
        "        \n",
        "        if os.path.isfile(file_path) is False:\n",
        "            raise ValueError(f\"Input file path {file_path} not found\")\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(f\"Creating features from dataset file at {file_path}\")\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            \n",
        "            #lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "            lines = []\n",
        "            text = f.read()\n",
        "            start_index = text.find(\"<BOS>\", 0)\n",
        "            while (start_index < len(text)):\n",
        "              end_index = text.find(\"<EOS>\", start_index) + 5\n",
        "              screenplay = text[start_index:end_index]\n",
        "              lines.append(screenplay)\n",
        "              while end_index < len(text) and text[end_index] == \"\\n\":\n",
        "                end_index += 1\n",
        "              start_index = end_index\n",
        "\n",
        "\n",
        "        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n",
        "        self.examples = batch_encoding[\"input_ids\"]\n",
        "        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n",
        "        return self.examples[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Od_Gxsz1hZvV"
      },
      "outputs": [],
      "source": [
        "# Create LineByLineDataset from Movie Plots text file\n",
        "def get_dataset(\n",
        "    args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False\n",
        "):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return BosToEosTextDataset(\n",
        "            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size\n",
        "        )\n",
        "    else:\n",
        "        return TextDataset(\n",
        "            tokenizer=tokenizer,\n",
        "            file_path=file_path,\n",
        "            block_size=args.block_size,\n",
        "            overwrite_cache=args.overwrite_cache,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXE6Zq8-hikF",
        "outputId": "4abe12aa-dd86-4315-da93-1320d414cd44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    model_args = ModelArguments(\n",
        "        model_name_or_path=\"gpt2\", model_type=\"gpt2\"\n",
        "    )\n",
        "    data_args = DataTrainingArguments(\n",
        "        train_data_file=\"train-set-stripped.txt\",\n",
        "        eval_data_file=\"dev-set-stripped.txt\",\n",
        "        line_by_line=True,\n",
        "        block_size=512,\n",
        "        overwrite_cache=True,\n",
        "    )\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"story_generator_checkpoint\",\n",
        "        overwrite_output_dir=True,\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        #evaluate_during_training=False,\n",
        "        logging_steps=500,\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=5,\n",
        "        save_total_limit=1,\n",
        "        save_steps=1000,\n",
        "    )\n",
        "\n",
        "    if data_args.eval_data_file is None and training_args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "\n",
        "    if (\n",
        "        os.path.exists(training_args.output_dir)\n",
        "        and os.listdir(training_args.output_dir)\n",
        "        and training_args.do_train\n",
        "        and not training_args.overwrite_output_dir\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
        "        )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        training_args.local_rank,\n",
        "        training_args.device,\n",
        "        training_args.n_gpu,\n",
        "        bool(training_args.local_rank != -1),\n",
        "        training_args.fp16,\n",
        "    )\n",
        "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
        "\n",
        "    # Set seed for deterministic training runs\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.model_name_or_path, cache_dir=model_args.cache_dir\n",
        "    )\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "    )\n",
        "\n",
        "    t = open(\"special-tokens.txt\", \"r\")\n",
        "    tokens_init = t.readlines()\n",
        "    tokens = [token.strip() for token in tokens_init]\n",
        "\n",
        "    special_tokens_dict = {\n",
        "        \"bos_token\": \"<BOS>\",\n",
        "        \"eos_token\": \"<EOS>\",\n",
        "        \"pad_token\": \"<PAD>\",\n",
        "        \"additional_special_tokens\": tokens,\n",
        "    }\n",
        "\n",
        "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if data_args.block_size <= 0: \n",
        "      # If block_size <= 0, set it to max. possible value allowed by model\n",
        "        data_args.block_size = tokenizer.model_max_length\n",
        "    else:\n",
        "        data_args.block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "    # Get datasets\n",
        "\n",
        "    train_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None\n",
        "    )\n",
        "    eval_dataset = (\n",
        "        get_dataset(data_args, tokenizer=tokenizer, evaluate=True)\n",
        "        if training_args.do_eval\n",
        "        else None\n",
        "    )\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=data_args.mlm,\n",
        "    )\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        #prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "      if training_args.do_train:\n",
        "          model_path = (\n",
        "              model_args.model_name_or_path\n",
        "              if model_args.model_name_or_path is not None\n",
        "              and os.path.isdir(model_args.model_name_or_path)\n",
        "              else None\n",
        "          )\n",
        "          trainer.train(model_path=model_path)\n",
        "          trainer.save_model()\n",
        "          tokenizer.save_pretrained(training_args.output_dir)\n",
        "    except KeyboardInterrupt:\n",
        "      print(\"Saving model that was in the middle of training\")\n",
        "      trainer.save_model()\n",
        "      tokenizer.save_pretrained(training_args.output_dir)\n",
        "      return\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        eval_output = trainer.evaluate()\n",
        "\n",
        "        perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "        result = {\"perplexity\": perplexity}\n",
        "\n",
        "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results_lm.txt\")\n",
        "        if trainer.is_world_process_zero():\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results *****\")\n",
        "                for key in sorted(result.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "        results.update(result)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RdzdlR4ciaZ6",
        "outputId": "a54d98f7-91fb-40d7-ba47-c13da36bd4bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "02/26/2022 05:28:14 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/26/2022 05:28:14 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=story_generator_checkpoint/runs/Feb26_05-28-14_f99f34adca75,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=story_generator_checkpoint,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=story_generator_checkpoint,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Assigning <BOS> to the bos_token key of the tokenizer\n",
            "Assigning <EOS> to the eos_token key of the tokenizer\n",
            "Assigning <PAD> to the pad_token key of the tokenizer\n",
            "Assigning ['<David Ward>', '<Ira Sachs>', '<Neal Purvis>', '<David Ayer>', '<David Lindsay-Abaire>', '<Clark Gregg>', '<William Blinn>', '<Hayden Schlossberg>', '<Alexander Dinelaris Jr.>', '<Claude- Michel Schonberg>', '<Marc Norman>', '<Joel Soisson>', '<John J. Strauss>', '<David Marconi>', '<John Krasinski>', '<Walon Green>', '<Fax Bahr>', '<J.D. Zeik>', '<Neal Israel>', '<James Cameron>', '<Armando Bo>', '<Alexandre Rockwell>', '<Steven Zaillian>', '<Bryan Woods>', '<Matthew Sullivan>', '<Howard Sackler>', '<Paul Brodeur>', '<Jason Filardi>', '<Alphonse Allais>', '<Daniel Boulanger>', '<Neil Cuthbert>', '<Michel Gondry>', '<Richard Jessup>', '<Milo Addica>', '<Larry Ferguson>', '<Robert Mark Kamen>', '<Tom Holland>', '<David Baldacci>', '<Andrew W. Marlowe>', '<Anthony Peckham>', '<Ann Druyan>', '<David Michod>', \"<Bridget O'Connor>\", '<Mark Andrus>', '<Amy Heckerling>', '<Bert Kelmer>', '<Derek Cianfrance>', '<Bill Condon>', '<Michel Audiard>', '<Ring Lardner Jr>', '<Adrian Molina>', '<Dee Rees>', '<Bert V. Royal>', '<Abby Kohn>', '<Steven Lisberger>', '<Bob Gale>', '<Ruth Rose>', '<David Ebershoff>', '<Tracey Jackson>', '<Tonino Guerra>', '<Wes Anderson>', '<Alfonso Cuaron>', '<Robert C. Jones>', '<Frances Goodrich>', '<Dennis Lee>', '<John Norville>', '<Richard Kelly>', '<Allison Anders>', '<Seth Macfarlane>', '<Bruce Robinson>', '<Victor Miller>', '<Orson Welles>', '<Ronald Moore>', '<Ernest Tidyman>', '<Scott Spiegel>', '<Jonathan Nolan>', '<Fred Walton>', '<Nicole Kassell>', '<Scott Smith>', '<John Romano>', '<Mark Jacobson>', '<Andrea Berloff>', '<Euzhan Palcy>', '<Giuseppe Tornatore>', '<John Ridley>', '<Ethan Edwards>', '<Bill Forsyth>', '<Alex Proyas>', '<Waldo Salt>', '<Tony Randel>', '<Ross Helford>', '<Tom Robbins>', '<Carl Franklin>', \"<Dan O'Bannon>\", '<Andy Wachowski>', '<George Axelrod>', '<Gerald Brach>', '<C. Gaby Mitchell>', '<Ben Stiller>', '<Steve Levitt>', '<Pamela Gray>', '<Larysa Kondracki>', '<Jason Reitman>', '<Frank Herbert>', '<Horror>', '<Andrew Davis>', '<Richard Curtist>', '<Brian Koppelman>', '<Tim Herlihy>', '<Guillermo Arriaga>', '<Peter Steinfeld>', '<Travis Wright>', '<Roald Dahl>', '<Tony Roche>', '<Charles Brackett>', '<David Benioff>', '<Benh Zeitlin>', '<John William Corrington>', '<Kate Angelo>', '<Jessie Nelson>', '<Terry Hayes>', '<Richard Rush>', '<Julius J. Epstein>', '<Don Roos>', '<Graham Yost>', '<Justin Theroux>', '<Christopher Wilkinson>', '<Shane Acker>', \"<Mark O'Keefe>\", '<Philip Eisner>', '<George Lucas>', '<Eric Stuyvesant>', '<Jerry Zucker>', '<Julius Epstein>', '<Roger Kumble>', '<Carl Ellsworth>', '<Michael Ferris>', '<Alex Garland>', '<Nick Villiers>', '<Clive Barker>', '<Mark Heyman>', '<John Hughes>', '<Pedro Almodovar>', '<Melisa Wallack>', '<John Landis>', '<Tom Rickman>', '<Paul Haggis>', '<Rob Cowan>', '<Mark Boal>', '<Dean DeBlois>', '<Daniel Waters>', '<Michael France>', '<Jerusha Hess>', '<Tom Clancy>', \"<James O'Barr>\", '<Ryan Coogler>', '<Anthony Minghella>', '<David Seidler>', '<Jennifer Jason Leigh>', '<Kem Nunn>', '<Mandy Nelson>', '<Mardik Martin>', '<Gordon Ray Young>', '<James Kearns>', '<Hershel Weingrod>', '<Terry Southern>', '<Gale Anne Hurd>', '<Peter Seaman>', '<Bruce Feirstein>', '<Jay Presson Allen>', '<James Sallis>', '<C.S. Lewis>', '<Christopher Landon>', '<Jim Lovell>', '<J.C. Chandor>', '<John Carpenter>', '<James L. Brooks>', '<Mike Figgis>', '<Eric Johnson>', '<Elizabeth Chandler>', '<George Clooney>', '<Roderick Taylor>', '<Raymond Chandler>', '<Sacha Gervasi>', '<Bob Guza>', '<Ricky Gervais>', '<John August>', '<James V. Hart>', '<Pamela Wallace>', '<Donald Kaufman>', '<War>', '<Norman Panama>', '<Samuel Taylor>', '<William Shakespeare>', '<Spike Jonze>', '<Agnieszka Wojtowicz-Vosloo>', '<Pen Densham>', '<Ted Lewis>', '<Andrew Kevin Walker>', '<John Cusack>', '<Western>', '<Emile Gaudreault>', '<Richard Condon>', '<Scott Abbott>', '<Brad Mirman>', '<Atom Egoyan>', '<Aaron Stockard>', '<Anthony Yerkovich>', '<Tim Day>', '<Emma Thompson>', '<Jeff Fradley>', '<Jay Cocks>', '<Bernhard Schlink>', '<Ken Daurio>', '<Robert Venditti>', '<Timothy Harris>', '<Kenneth Longeran>', '< David Zucker>', '<Len Blum>', '<Josh Trank>', '<Emma Donoghue>', '<Howard Franklin>', '<David E Kelley>', '<>', '<Charles Gilliam>', '<Robert Engels>', '<Stephen Sondheim>', '<Sabi H. Shabtai>', '<Dean Devlin>', '<Pamela Pettler>', '<Brent Simons>', '<Matthew Sand>', '<Neil Jordan>', '<Jennifer Lee>', '<Sofia Coppola>', '<Randy Feldman>', '<Jack B Sowards>', '<Joel Cohen>', '<Joey Curtis>', '<Billy Bob Thornton>', '<Donald E. Westlake>', '<Robert Ben Garant>', '<Alan Ball>', '<Dan Fogelman>', '<Brent Maddock>', '<Jordan Belfort>', '<Steve Tesich>', '<Peter Briggs>', '<Gillo Pontecorvo>', '<Megan Holley>', '<Marshall Brickman>', '<Jon Lucas>', '<David Self>', '<Roger Spottiswoode>', '<Irwin Allen>', '<Eric Warren Singer>', '<John McLaughlin>', '<Alain Boublil>', '<John Watson>', '<Allan Loeb>', '<Alvin Sargent>', '<Lucy Alibar>', '<Danny Rubin>', '<Mike Werb>', '<Scott Alexander>', '<Rafael Moreu>', '<Craig Bolotin>', '<Patrick Duncan>', '<Karl Gajdusek>', '<Jeremy Brock>', '<Joseph Kesselring>', '<Gregory Levasseur>', '<Tony Giglio>', '<Peter Buchman>', '<Eric Bergren>', '<Irena Brignull>', '<Charles Randolph>', '<Ethan Coen>', '<Nicholas Kazan>', '<Robert Pulcini>', '<Joan Tewksbury>', '<Evan Daugherty>', '<Fran Walsh>', '<Steven Fechter>', '<Rob Siegel>', '<Michael Hirst>', '<Abi Morgan>', '<David Magee>', '<Ted Griffin>', '<Bob Tzudiker>', '<Adventure>', '<Matthew Stone>', '<Lem Dobbs>', '<Clarence Budington Kelland>', '< Ellory Elkayem>', '<Douglas Day Stewart>', \"<David O'Malley>\", '<Russell Banks>', '<Josann McGibbon>', '<David Cronenberg>', '<Joseph Delteil>', '<Jack Golden Russell>', '<Adam Pava>', '<Alan Cummings>', '<Gregory Widen>', '<Mauricio Zacharias>', '<Stephen Rebello>', '<Josh Klausner>', '<Steve Feke>', '<Peter Torokvel>', '<Daniel Farrands>', '<Daniel Clowes>', '<M. Night Shyamalan>', '<Mike Mills>', '<Jon Bokenkamp>', '<Amy Holden Jones>', '<Lawrence Konner>', '<John Collee>', '<Francis Ford Coppola>', '<Ron Bass>', '<Warren Skaaren>', '<Zach Braff>', '<David Scinto>', '<Alex Cox>', '<Mario Puzo>', '<Diane Johnson>', '<Steven Pressfield>', '<Kirk DeMicco>', '<Winston Miller>', '<Melvin Frank>', '<Margaret Nagle>', '<Cory Goodman>', '<Ronald Roose>', \"<John O'Brian>\", '<Carl V Dupre>', '<Duane G. Adler>', '<Allison Burnett>', '<Chris Provenzano>', '<Nicole Eastman>', '<Vince McKewin>', '<Franco Arcalli>', '<Ray Wright>', '<Melissa Mathison>', '<Clayton Frohman>', '<Chris Butler>', '<Baz Luhrmann>', '<Ann Eiderman>', '<Gary Whitta>', '<Nathan Atkins>', '<Bill Walsh>', '<Dave Barry>', '<Kurt Russell>', '<Philip Epstein>', '<Kevin Jarre>', '<William Monahan>', '<Maria Veltre Druse>', '<Ebbe Roe Smith>', '<Albert Magnoli>', '<Gary Goldman>', '<Graeme Manson>', '<Roman Coppola>', '<Joe Penhall>', '<Mark Bomback>', '<Frances Walsh>', '<Patrick Melton>', '<Josh Heald>', '<David Webb Peoples>', '<John C. Richards>', '<Matthew Robinson>', '<Joel Coen>', '<Trey Callaway>', '<Glenn Ficarra>', '<Anthony Waller>', '<Justin Haythe>', '<Joseph Mankiewicz>', '<Sonny Mallhi>', '<Adam Green>', '<Terry Gilliam>', '<Danny Wallace>', '<Tom Stoppard>', '<Philip K Dick>', '<Caspian Tredwell-Owen>', '<Matthew Vaughn>', '<Sam Peckinpah>', '<Erich Hoeber>', '<John Hodge>', '<Jon Ronson>', '<Ronald Harwood>', '<Adam Herz>', '<Travis Milloy>', '<David Rabinowitz>', '<Mel Brooks>', '< D.V. deVincentis>', '<Christopher Grimm>', '<David Veloz>', '<David Peoples>', '<Steven E. De Souza>', '<Judith Guest>', '<Emily Fox>', '<Brad Copeland>', '<Anne Rice>', '<Rian Johnson>', '<Steven Brill>', '<Jim Uhls>', '<James Gray>', '<Richard Tuggle>', '<Jon Hoeber>', '<Michael Tolkin>', '<Felix Sutton>', '<Nicolas Winding Refn>', '<Burleigh Smith>', '<John L. Balderston>', '<Michael Diliberti>', '<Bruce Joel Rubin>', '<Andrew Niccol>', '<Stephenie Meyer>', '<Harry Brown>', '<Colline Serreau>', '<Larry Wachowski>', '<Nathan Alexander>', '<Robert Roy>', '<Matthew Chapman>', '<William N. Panzer>', '<Patrick Lussier>', '<Ed Solomon>', '<Jerry Lewis>', '<Christopher L. Yost>', '<Joel Hopkins>', '<Tom Stern>', '<Arlene Sarner>', '<Douglas W. Churchill>', '<Beau Michael Thorne>', '<Andrew Bergman>', '<Andrew Neiderman>', '<Nina Wilcox Putnam>', '<Harald Kloser>', '<Martin Scorsese>', '<Jonathan Lemkin>', '< Action>', '<Martin Curland>', '<Chris Rock>', '<Terence Malick>', '<Neil Gaiman>', '<Derek Haas>', '<Carl Theodor Dreyer>', '<Bill Kelly>', '<Tom DeSanto>', '<Michael Cristofer>', '<Noah Baumbach>', '<Robert A. Heinlein>', '<Tom Schulman>', '<Carol Watson>', '<Evan Goldberg>', '<Matt Greenberg>', '<Paul Thomas Anderson>', '<Richard Lagravenese>', '<David Lewis>', '<Wayne Kramer>', '<Steven Spielberg>', '<Chris Columbus>', '<J.G. Ballard>', '<Jim Taylor>', '<Mike Binder>', '<George A. Romero>', '<Mark Protosevich>', '<Scott Seeke>', '<Michael Haneke>', '<Paul B. Margolis>', '<Jerry Kosinski>', '<Ron Friedman>', '<Animation>', '<Michael Mann>', '<Stuart Blumberg>', '<Douglas Adams>', '<Ernest Lehman>', '<Jared Hess>', '<Larry Gross>', '<Clifford Odets>', '<Guy Ritchie>', '<Lou Holtz Jr.>', '<Dennis Shryack>', '<Terry Zwigoff>', '<W. Peter Iliff>', '<Quentin Tarantino>', '<Brad Bird>', '<Robert Getchell>', '<William Hjortsberg>', '<Jeff Reno>', '<William Wisher Jr.>', '<David Hayter>', '<Ben Queen>', '<Tony Gilroy>', '<Chip Johannessen>', '<François Boyer>', '<David Twohy>', '<Oliver Stone>', '<Kim Barker>', '<Marty Kaplan>', '<James Hilton>', '<J Michael Straczynski>', '<Jonathan Aibel>', '<Bryan Singer>', '<Kevin Smith>', '<Todd Phillips>', '<Arturo Pérez-Reverte>', '<Tim Burns>', '<Richard Linklater>', '<Vincenzo Natali>', '<Marshall Herskovitz>', '<Paul Wernick>', '<John Rogers>', '<Jon Krakauer>', '<Emily V. Gordon>', '<Henry Bean>', '<Lauren Weisberger>', '<Alan Schoolcraft>', '<Kathryn Bigelow>', '<Terry McKeown>', '<Andy Lewis>', '<Frank Nugent>', '<Dan Gordon>', '<Yves Robert>', '<Peter Atkins>', '<Pete Jones>', '<Stephen Gaghan>', '<Marlon Wayans>', '<Robert Rodriguez>', '<Eric Roth>', '<Hillary Seitz>', '<Frank Baldwin>', '<Stanley Weiser>', '<Pete Chiarelli>', '<Herbert Kretzmer>', '<Anthony McCarten>', '<Alejandro G. Inarritu>', '<Pete Docter>', '<Calder Willingham>', '<Frederic Raphael>', '< S.K. Boatman>', '<Ross LaManna>', '<Jon Schroder>', '<Warren Lewis>', '<Drama>', '<Todd Farmer>', '<Ken Finkleman>', '<Jamie Delano>', '<Bernard Murray Jr>', '<Cami Delavigne>', '<Brian Aldiss>', '<Tom Hanks>', '<Lawrence Dworet>', '<Steve McQueen>', '<David Seltzer>', '<Charles MacArthur>', '<Larry Clark>', '<David Mamet>', '<Roland Emmerich>', '<Billy Ray>', '<Michael Lewis>', '<Howard Koch>', '<Andrew Jones>', '<Matthew Aldrich>', '<Larry Karaszewski>', '<Arnold Perl>', '<Leigh Brackett>', '<John Briley>', '<Don DeLillo>', '<Sean Anders>', '<Ice Cube>', '<J. Michael Straczynski>', '<Barry Gifford>', '<Richard LaGravenese>', '<Warren Beatty>', '<John Gatins>', '<Rick Jaffa>', '<Michael Miner>', '<Walter Hill>', '<Peter Hewitt>', '<David Giler>', \"<Rockne O'Bannon>\", '<Jonathan Roberts>', '<Craig Kyle>', '<Rich Wilkes>', '<Harry E. Chandlee>', '<Melissa Rosenberg>', '<Garth Ennis>', '<Michael H. Weber>', '<Stephen J Rivele>', '<Kurt Eichenwald>', '<Carl Dupre>', '<Jim Kouf>', '<Brannon Braga>', '<Albert Simonin>', '<Judd Apatow>', '<Lawrence Kasda>', '<Chase Palmer>', '<Frank Pierson>', '<Chris Sanders>', '<John W. Campbell Jr.>', '<Harry Ruby>', '<Danny Boyle>', '<Scot Armstrong>', '<David Leslie Johnson>', '<E. Max Frye>', '<Justin Simien>', '<Peter Hume>', '<Scott Yagemann>', '<John Lutz>', '<David Congalton>', '<Michael Wilson>', '<Patrick Sheane Duncan>', '<Jean Marsan>', '<Steve Kloves>', '<Owen Wilson>', '<Harlan Ellison>', '<Wes Craven>', '<Susan Montford>', '<Ken Hixon>', '<Eilis Kirwan>', '<Thomas Dean Donnelly>', '<Doug Jung>', '<Mystery>', '<John Kamps>', '<Craig Pearce>', '<Vince Gilligan>', '<Robert Riskin>', '<Jim Thomas>', '<Lona Williams>', '<Vikas Swarup>', '<Antoine Blondin>', '<Tate Taylor>', '<David Klass>', '<Gloria Katz>', '<Keir Pearson>', '<Scott Beck>', '<Chris Miller>', '<Jane Goldman>', '<Mark Hanlon>', '<Kevin Williamson>', '<Susan Gauthier>', '<Frank Darabont>', '<Aaron Sorkin>', '<Michael Herzberg>', '<Patrick Marber>', '<Robert D. San Souci>', '<Diablo Cody>', '<Eric Bress>', '<Lee Eisenberg>', '<Masamitsu Hidaka>', '<Cressida Cowell>', '<Robert Rodat>', '<Wayne Powers>', '<Jim Rash>', '<Robert Benton>', '<David N. Twohy>', '<Richard Jefferies>', '<Evan Spiliotopoulos>', '<Stuart Beattie>', '<Wang Hui Ling>', '<Lawrence Bridges>', '<John Michael Hayes>', '<Daphne Du Maurier>', '<Rob Hedden>', '<Andrew Stanton>', '<Ann Biderman>', '<Robert Crais>', '<Jim Cruickshank>', '<Peter Shaffer>', '<Jeff Nathanson>', '<Ted Tally>', '<James Ponsoldt>', '<Ron Osborn>', '<Jonas Cuaron>', '<Music>', '<Buck Henry>', '<Herman J. Mankiewicz>', '<Nick Guthe>', '<Bonnie MacBird>', '<Jeremy Drysdale>', '<James M. Cain>', '<David Aaron Cohen>', '<Edmund H. North>', '<Brian Taylor>', '<Matthew Robbins>', '<David Lynch>', '<Bob Peterson>', '<Cary Fukunaga>', '<Jenny Lumet>', '<Irvine Welsh>', '<Lawrence Lasker>', '<Michael Ende>', '<Lucinda Coxon>', '<Robert Gunter>', '<Mike Gray>', '<Jeffrey Boam>', '<Greg Rucka>', '<Robert Festinger>', '<Joseph Stefano>', '<Spencer Susser>', '<Gillian Horvath>', '<Terry Rossio>', '<Abel Ferrara>', '<Lorenzo Semple Jr.>', '<John Morris>', '<J.J. Abrams>', '<Jesse Alexander>', '<Gus Van Sant>', '<Elaine May>', '<Chris Carter>', '<Roland Joffe>', '<Sam Raimi>', '<Michael Pye>', '<Tsai Kuo Jung>', '<Chris Weitz>', '<Kevin Lehane>', '<Adam Rockoff>', '<Thomas Lennon>', '< Comedy>', '<Don McGuire>', '<Alec Coppel>', '<John Glenn>', '<David Newman>', '<Kumail Nanjiani>', '<J.K. Rowling>', '<Paddy Chayefsky>', '<Leslie Dixon>', '<Henry Selick>', '<Alex Tse>', '<John Logan>', '<Don DaGradi>', '<Richard Outten>', '<Nick Frost>', '<Biography>', '<Michael McCullers>', '<David Loughery>', '<Willard Huyck>', '<Bob Kane>', '<Elizabeth Meriwether>', '<Menno Meyjes>', '<Jason Fulardi>', '<Mark Neveldine>', '<Nancy Meyers>', '<Gigi Levangie>', '<Gene Roddenberry>', '<William Nicholson>', '<Janet Peoples>', '<Dan Futterman>', '<Robert Bloch>', '<John Lee Hancock>', '<Steven Seagal>', '<Florence Ryerson>', '<John Mason>', '<Eran Creevy>', '<Jim Marrs>', '<Michael Slone>', '<Scott Moore>', '<Ian Watson>', '<Pierre Bismuth>', '<Scott Rosenberg>', '<Sebastian Gutierrez>', '<Jack Sekowski>', '<William Broyles Jr.>', '<David Goyer>', '<Christopher Crowe>', '<Paul Tamasy>', '<Paul Mayersberg>', '<Massy Tadjedin>', '<Phil Johnston>', '<Horror.Mystery>', '<Glenn Berger>', '<John Requa>', '<Sarah Thorp>', '<Rospo Pallenberg>', '<Michael Walker>', '<J. Mackye Gruber>', '<Hans Christian Andersen>', '<Leo Marks>', '<Earl W. Wallace>', '<Dean Georgaris>', '<Amanda Silver>', '<Terrance Malick>', '<Paul Attanasio>', '<Chris Bertolini>', '<David Andalman>', '<Peter S. Seaman>', '<David McKenna>', '<Yann Martel>', '<Louis Mellis>', '<Zachary Sklar>', '<Keith Strandburg>', '<Jeffrey Kluger>', '<Joe Stillman>', '<Neal Marshall Stevens>', '<Billy Hayes>', '<Adam Sandler>', '<Duncan Jones>', '<Julian Schnabel>', '<Alex Winter>', '<Tom Jankiewicz>', '<Peter Stone>', '<Steve Oedekerk>', '<Craig Brewer>', '<Bernardo Bertolucci>', '<Tracy Letts>', '<Georges Lautner>', '<Seth Rogen>', '<Pam Brady>', '<Jim Garrison>', '<Spike Lee>', '<Charlie Wachtel>', '<James Vanderbilt>', '<Jacques Celhay>', '<William Hoffer>', '<Steve Koren>', '<George Nolfi>', '<Dan Aykroyd>', '<Rob Zombie>', '<Noel Langley>', '<Thomas McCarthy>', '<Christopher Nolan>', '<Jason Keller>', '<Annie Mumolo>', '<Slavomir Rawicz>', '<Kelly Marcel>', '<Cary Joji Fukunaga>', '<Seth Lochead>', '<Glenn Standring>', '<Ben Hecht>', '<Sean Penn>', '<Misan Sagay>', '<James Ellroy>', '<Todd Field>', '<Karen Walton>', '<Al Reinert>', '<Charles Addams>', '<Geoffrey Fletcher>', '<Scott Z. Burns>', '<Ian Fleming>', '<Kathryn Stockett>', '<Scott Neustadter>', '<Sean Durkin>', '<Marc Rocco>', '<Roger Allers>', '<Susannah Grant>', '<Mike Judge>', '<Jacques Audiard>', '<Steven Baigelman>', '<Anne Spielberg>', '<Sheldon Turner>', '<Leigh Whannell>', '<Bobby Farrelly>', '<Crime>', '<Melchior Lengyel>', '<Cameron Crowe>', '<Jonathan Hirschbein>', '<Trey Parker>', '<Earl Mac Rauch>', '<Noni White>', '<David Weissman>', '<Aeysha Carr>', '<Russel Banks>', '<Shane Black>', '<Andres Heinz>', '<Josh Friedman>', '<Harve Bennett>', '<Barry Reed>', '<Joshua Goldin>', '<Grover Jones>', '<David H. Steinberg>', '<Evan Hunter>', '<Harmony Korine>', '<Steven Sommers>', '<Robbie Fox>', '<Czenzi Ormonde>', '<Larry Cohen>', '<Sport>', '<Tony Kushner>', '<Lawrence B. Marcus>', '<William Davies>', '<Courtney Hunt>', '<Jo Swerling>', '<David Peace>', '<Hal Hartley>', '<Michael Barrie>', '<Robert Wade>', '<Bela Balazs>', '<Anthony Cipriano>', '<Scott Stewart>', '<Anna Boden>', '<Jim Henson>', '<Ed Decter>', '<Tim Metcalfe>', '<Thomas Bidegain>', '<Dustin Lance Black>', '<Sam Hamm>', '<Gary Scott Thompson>', '<Terry Jones>', '<Alec Sokolow>', '<Michael Herr>', '<Stephen Sommers>', '<Caroline Thompson>', '<Rhett Reese>', '<Gustin Nash>', '<Cornell Woolrich>', '<Julian Fellowes>', \"<Gavin O'Connor>\", '<Todd Alcott>', '<Bill Lancaster>', '<Cheryl Edwards>', '<Jeff Arch>', '< Jason Hall>', '<Ted Elliot>', '<James Lapine>', '<Randall Wallace>', '<Jim Abrahams>', '<Howard Breslin>', '<Hugh Wilson>', '<David S. Ward>', '<Zack Snyder>', '<Sci-Fi>', '<Rowan Joffe>', '<Kenneth Lonergan>', '<David Fallon>', '<Frank Fenton>', '<S. S. Wilson>', '<Ron Nyswaner>', '<Stewart Stern>', '<Ken Selden>', '<William Osborne>', '<Walter Mosley>', '<Millard Kaufman>', '<George Clayton Johnson>', '<Thomas Harris>', '<Robert Ludlum>', '<Michael Petroni>', '<Jeffrey Price>', '<Ron Clements>', '<Joseph Kosinski>', '<Joe Robert Cole>', '<Jim Mulholland>', '<John Krokidas>', '<Jim Carrey>', '<Bob Israel>', '< Bennett Yellin>', '<John Thomas>', '<Peter Tolan>', '<Richard Matheson>', '<Nicholas Jarecki>', '<Peter Viertel>', '<Peter Schink>', '<Roman Polanski>', '<Takahashi Hiroshi>', '<Harold Ramis>', '<Debra Hill>', '<Luc Besson>', '<John-Henry Butterworth>', '<Dan Goldberg>', '<Walter Reisch>', '<Will Reiser>', '<Zak Penn>', '<Philip Kaufman>', '<Winston Groom>', '<Daniel Taradash>', '<Randall Jahnson>', '<Frank Hannah>', '<Margery Sharp>', '<Joss Whedon>', '<Irwin Winkler>', '<Nat Faxon>', '<Jeb Stuart>', '<Peter Jackson>', '<Ashley Miller>', '<Brandon Boyce>', '<Musical>', '<Howard Ashman>', '<D.M. Harshman Jr.>', '<Joe Eszterhas>', '<Adam Simon>', '<John Waters>', '<Hal Barwood>', '<Carole Eastman>', '<Michael Brandt>', '<Michael Goldenberg>', '<Zoe Lund>', '<Nunnally Johnson>', '<Edward Bryant>', '<Kevin Barnett>', '<Frank Capra>', '<Drew Goddard>', '<René Clément>', '<Rex Pickett>', '<D.V. DeVincentis>', '<Michael Robert Johnson>', '<Action>', '<Jason Katz>', '<Christopher Columbus>', '<Philippa Boyens>', '<Oliver Sacks>', '<Jonathan Hensleigh>', '<Nathan Parker>', '<James Leo Herlihy>', '<David Zelgan Goodman>', '<Michelangelo Antonioni>', '<Ted Elliott>', '<John Orloff>', '<Michael Connelly>', '<Stanley Kubrick>', '<Ken Kwapis>', '<Eric Pearson>', '<Chris Sparling>', '<Robert Nelson Jacobs>', '<Lee Hall>', '<Kristine Johnson>', '<Ronan Bennett>', '<Kirsten Smith>', '<Simon Kinberg>', '<E. Annie Proulx>', '<Matt Damon>', '<Christopher McQuarrie>', '<Will Rokos>', '<James Orr>', '<Karen Janszen>', '<Film-Noir>', '<Carl Gottlieb>', '<Susan Burke>', '<Kurt Vonnegut Jr.>', '<Romance>', '<Charles McKeown>', '<Luke Short>', '<Didier Kaminka>', '<Leonard Schrader>', '<Tom McCarthy>', '<I.A.L Diamond>', '<David Schow>', '<Laurice Elehwany>', '<Cliff Dorfman>', '<Dan Pyne>', '<John Brownjohn>', '<William Fetters>', '<Action.Thriller>', '<Hugo Butler>', '<Jean Aurenche>', '<Stephen Peters>', '<Michael Colleary>', '<Vincent Ngo>', '<Toni-Ann Johnson>', '<David Hare>', '<Tony Grisoni>', '<Akiva Goldsman>', '<Richard Curtis>', '<Miles Millar>', '<Joshua Oppenheimer>', '<Simon Pegg>', '<Michael Arndt>', '<Jeff Nichols>', '<Lisa Cholodenko>', '<Edgar Wright>', '<Matt Reeves>', '<Derek Kolstad>', '<Jared Bush>', '<Bob Dolman>', '<Charlie Kaufman>', '<William Gray>', '<Tom Shadyac>', '<Bruce Graham>', '<Andrew Fleming>', '<Gina Prince-Bythewood>', '<Philip G. Epstein>', '<Lawrence Hauben>', '<Charles Denton>', '<Charles Burnett>', '<Chuck Pfarrer>', '<Michael Hoffman>', '<Nora Ephron>', '<Michael Bacall>', '<Jamie Moss>', '<Phil A. Robinson>', '<Jonathan Kesselman>', '<Matthew Michael Carnahan>', '<Peter Weir>', '<Wolfgang Petersen>', '<Paul D. Zimmerman>', '<Stephen Hauser>', '<Richard Connell>', '<Michael Blake>', '<Jerry Bicks>', '<David Mickey Evans>', '<Randall Rubin>', '<Jason Fuchs>', '<Deborah Moggach>', '<Oliver Butcher>', '<Channing Gibson>', '<Guillermo del Toro>', '<Roberto Orci>', '<Lewis John Carlino>', '<Nick Griffin>', '<John Milius>', '<Roger Avary>', '<Alexander Payne>', '<Edgar Allen Woolf>', '<Max Landis>', '<Kevin Willmott>', '<Sara Parriott>', '<Mike Hodges>', '<Woody Allen>', '<Joel Schumacher>', '<William Goldman>', '<Chris Morgan>', '<Elio Bartolini>', '<Steven Kloves>', '<Peter Berg>', '<Deric Washburn>', '<Ol Parker>', '<Nick Castle>', '<Pat Proft>', '<Michael Green>', '<Terence Winter>', '<Aline Brosh McKenna>', '<Glen Morgan>', '<Karey Kirkpatrick>', '<Mark Frost>', '<Ethan Cohen>', '<Andy Hurst>', '<Stephen Susco>', '<Brendan Hood>', '<John Patrick Shanley>', '<Mike Mignola>', '<Stephen King>', '<Don Macpherson>', '<Tom Zuber>', '<Hilary Seitz>', '<James Wan>', '<John Irving>', '<Peter Farrelly>', '<John Monk Saunders>', '<Matt Stone>', '<Todd Robinson>', '<Wladyslaw Szpilman>', '<Danny McBride>', '<John Musker>', '<John Pielmeier>', '<David O. Russell>', '<Daniel Goldin>', '<Toni Morrisson>', '<Shawn Wayans>', '<Nick Green>', '<Erica Beeney>', '<Nick Cassavetes>', '<William Kelley>', '<Grant Heslov>', '<Nicolas Giacobone>', '<Matthew Harrison>', '<Taylor Sheridan>', '<Scott Frank>', '<Michael Hogan>', '<Nick Swardson>', '<Jeff Zuber>', '<Lawrence Kasdan>', '<Pierre Bost>', '< Jim Sharman>', '<William Wisher>', '<Simon Beaufoy>', '<Danilo Bach>', '<Lowell Cannon>', '<Mariko Munro>', '<Robert Ramsey>', '<Ron Shelton>', '<James Flamberg>', '<Dan Kay>', '<Charlie Haas>', '<David Diamond>', '<Cinco Paul>', '<Nick Schenk>', '<Paul Weitz>', '<Marc Silverstein>', '<Collin Friesen>', '<Paul P Fix>', '<Charles Portis>', '<Joseph Conrad>', '<Charles Frazier>', '<Marcus Dunston>', '<Steven Rogers>', '<Claude Sautet>', '<Jane Campion>', '<Robert Towne>', '<Michael Butler>', '<Family>', '<Matthew Quick>', '<Alex Kurtzman>', '<Larry Marcus>', '<Ingmar Bergman>', '<Michael Markowitz>', '<Michael Chiffer>', '<Tim Burton>', '<Beau Willimon>', '<Jesse Armstrong>', '<Robert Garland>', '<George Miller>', '<Bruce A. Evans>', '<Jim Cox>', '<Christopher De Vore>', '<Carl Kurlander>', '<Adrian Hodges>', '<Nicholas Stoller>', '<James Mangold>', '<Alec Sulkin>', '<Carl Sagan>', '<David Franzoni>', '<William Rose>', '<Scott Silver>', '<Alexandre Aja>', '<Lloyd Fonvielle>', '<Sylvie Bochard>', '<John Boorman>', '<Lee Unkrich>', '<Aron Warner>', '<Arne Olsen>', '<Peter Morgan>', '<Franco Solinas>', '<Norman Krasna>', '<Dan Gilroy>', '<John Guare>', '<Lawrence D. Cohen>', '<Gene Stupnitsky>', '<Edward Neumeier>', '<Moira Buffini>', '<Jeffrey Scott Richards>', '<Eric Bernt>', '<Mike White>', '<History>', '<John Godey>', '<Fantasy>', '<Lynn Barber>', '<Peter Straughan>', '<Nick Hornby>', '<Todd Solondz>', '<Jon Spaihts>', '<David S Goyer>', '<Joseph Gangemi>', '<Ben Affleck>', '<J.F. Lawton>', '<Peter Benchley>', '<Troy Kennedy-Martin>', '<Anna Hamilton Phelan>', '<Mark Verheiden>', '<Steven Antin>', '<Chris Terrio>', \"<Richard O'Brien>\", '<Jonathan Herman>', '<Ronald Bass>', '<Comedy>', '<Jan Parini>', '<Claude Zidi>', '<Ronald Shusett>', '<Mike Myers>', '<John Fusco>', '<Jamie Kennedy>', '<Ehren Kruger>', '<Gerald DiPego>', '<John Sayles>', '<Lowell Cunningham>', '<Mark Rosenthal>', '<Adam Small>', '<Toby Emmerich>', '<Richard Schayer>', '<Jody Hill>', '<Kaja Blackley>', '<Chip Proser>', '<Richard Rutowski>', '<Robert Zemeckis>', '<Katherine Fugate>', '<Karen McCullah Lutz>', '<Wellesley Wild>', '<Damien Chazelle>', '<Scott Hicks>', '<Brian Helgeland>', '<Peter Baynham>', '<Thriller>', '<Paul Rudnick>', '<Ryan Fleck>', '<Austin Bunn>', '<Brian Hannant>', '<David Gordon Green>', '<Etan Cohen>', '<Arhur C. Clarke>', '<Mort Briskin>', '<John Brancato>', '<Jonathan Mostow>', '<David Odell>', '<James Wong>', '<Bo Goldman>', '<Ivan Raimi>', '<Donna Powers>', '<Raynold Gideon>', '<Jane Austen>', '<Rick Berman>', '<Herman Weigel>', '<Sylvester Stallone>', '<Westly Strick>', '<Tom Bleecker>', '<Charles Gieg Jr>', '<Jordan Peele>', '<William Steig>', '<Lawrence Wright>', '<Barry Levinson>', '<Zack Ordynans>', '<David S. Goyer>', '<Philippa Gregory>', '<James Gunn>', '<Phil Alden Robinson>', '<Darren Aronofsky>', '<Roslyne Bosch>', '<Troy Duffy>', '<Alfred Gough>', '<Tony Gayton>', '<Marti Noxon>', '<Irene Mecchi>', '<Zack Stentz>', '<John J. McLaughlin>', '<Tom DiCillo>', '<Phil Lord>', '<Robert Presnell Sr.>', '<Ben Ripley>', '<James Schamus>', '<André Bijelic>', '<Sarah Kernochan>', '<Steven Soderbergh>', '<Steph Lady>', '<Charles Lederer>', '<Doug Richardson>', '<Samuel Hopkins Adams>', '<Allan Heinberg>', '<Simon Michaël>', '<Geoffrey Homes>', '<Kevin Munroe>', '<Nicholas Pileggi>', '<Gary Ross>', '<Stephen Cornwell>', '<Billy Wilder>', '<Skip Woods>', '<Jon Hurwitz>', '<Robert Rossen>', '<Anthony Tambakis>', '<Rachel Talalay>', '<Harold Livingston>', '<Sue Smith>', '<Joe Carnahan>', '<Robin Briscoll>', '<Shari Springer Berman>', '<Hampton Fancher>', '<Vanessa Chong>', '<Ken Kesey>', '<Kevin Wade>', '<Elan Mastai>', '<William Gibson>', '<Jerry Leichtling>', '<Jack Bernstein>', '<Adam Resnick>', '<Michel Hazanavicius>', '<Albert Hackett>', '<Ben Lewin>', '<Jonathan Frederick Lawton>', '<Mark L. Smith>', '<Craig Borten>', '<David Levien>', '<Steven E. de Souza>', '<Ed Wood>', '<Aaron Latham>', '<Paul Schrader>', '<Jez Butterworth>', '<Armen Evrensel>', '<Matthew Markwalder>', '<Hossein Amini>', '<Gustav Hasford>', '<Daniel Wallace>', '<Stephen Chbosky>', '<Armando Iannucci>', '<Delia Ephron>', '<David Koepp>', '<Steve Conrad>', '<Enrique Urbizu>', '<Edward Zwick>', '<John Hertzfield>', '<Tom Butterworth>', '<Rick King>', '<Steven Knight>', '<Michael Patrick King>', '<Steven Katz>', '<Michael Roesch>', '<Ann Peacock>', '<Jon Favreau>', '<Kurt Wimmer>', '<Short>', '<Robert Baer>', '<Simon Blackwell>', '<Terry George>'] to the additional_special_tokens key of the tokenizer\n",
            "02/26/2022 05:28:26 - INFO - __main__ -   Creating features from dataset file at train-set-stripped.txt\n",
            "02/26/2022 05:30:20 - INFO - __main__ -   Creating features from dataset file at dev-set-stripped.txt\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1102: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 935\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1170\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1170' max='1170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1170/1170 24:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.249400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.528200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to story_generator_checkpoint/checkpoint-1000\n",
            "Configuration saved in story_generator_checkpoint/checkpoint-1000/config.json\n",
            "Model weights saved in story_generator_checkpoint/checkpoint-1000/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to story_generator_checkpoint\n",
            "Configuration saved in story_generator_checkpoint/config.json\n",
            "Model weights saved in story_generator_checkpoint/pytorch_model.bin\n",
            "tokenizer config file saved in story_generator_checkpoint/tokenizer_config.json\n",
            "Special tokens file saved in story_generator_checkpoint/special_tokens_map.json\n",
            "02/26/2022 05:54:54 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 116\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "02/26/2022 05:55:06 - INFO - __main__ -   ***** Eval results *****\n",
            "02/26/2022 05:55:06 - INFO - __main__ -     perplexity = 5.687769033037744\n"
          ]
        }
      ],
      "source": [
        "# Press the Run Cell button to the left to start training\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# To stop training and save model, press the same Run Cell button (now, it is the Interrupt Execution button)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xQ4GPsSfQrDH"
      },
      "outputs": [],
      "source": [
        "# This cell is to style the Google Colab's output properly (Just blindly run this)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "AUoTHvmHHz43",
        "outputId": "409415d8-97c3-4475-932b-0874ca512375"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file story_generator_checkpoint/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.16.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51656\n",
            "}\n",
            "\n",
            "loading weights file story_generator_checkpoint/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at story_generator_checkpoint.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "loading file story_generator_checkpoint/vocab.json\n",
            "loading file story_generator_checkpoint/merges.txt\n",
            "loading file story_generator_checkpoint/tokenizer.json\n",
            "loading file story_generator_checkpoint/added_tokens.json\n",
            "loading file story_generator_checkpoint/special_tokens_map.json\n",
            "loading file story_generator_checkpoint/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "# Run these cells for story generation\n",
        "from transformers import pipeline, TextGenerationPipeline, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "\"\"\" \n",
        "Below, my model checkpoint is commented out. You can replace your checkpoint \n",
        "with that to test story generation if your checkpoint didn't train for long enough\n",
        "\"\"\"\n",
        "#checkpoint = \"pranavpsv/gpt2-genre-story-generator\"\n",
        "checkpoint = \"story_generator_checkpoint\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "story_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "# The format for input_prompt: \"<BOS> <genre> Optional text...\"\n",
        "# Supported genres: superhero, sci_fi, horror, thriller, action, drama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wxTcvA6bdB1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "id": "clXP7bNrPxja",
        "outputId": "ee364fc4-ac42-4b3e-c27f-f536435ca966"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': '<BOS><Thriller><J. K. Rowling>  How did you get your wand?\"\\n\\n\\nMr Potter looks up from his glasses; then back at Harry, looking to Ron\\'s face and frowning-- the image of a wiggly twelfth year boy standing next for years until it turns evil in front... but no one comes over here with anything other than laughter or grief.. just curiosity-seeking boys watching curiously on television playing cricket! A few rows away there are some fifty young wizards whose noses appear very wide against them: an elderly owl who wants more money does what he can but knows that this is going wrong if ever brought something by anybody? Another row outside they discover James sitting behind Ginny (who lies there talking loudly), smiling quietly while her cousin Fred sees him sit somewhere sleeping along beside Mrs Weasley when she gives way through shouting as though frightened weasels could cross their forearms onto themselves --\\n JAMES VOICE OF OILING THE EARLY NIGHT - The owls come crashing down into meadows like ants flying over trees... We all know Mr Muggle never left Hogwarts. And far too many others have been caught fighting with witches and goblins... and nobody knows where these old ladies were stationed; how exactly did I do any detective work yet again?\\nI hear Ron ask Ginny (not sure girl) why nothing was done to prevent us bringing Hermione, Susan... which has caused confusion among ourselves so badly.... She says about this last time not three hundred per cent will takeness,\"and think that intelligence goes hand only bag round\" [potion]... wise\"...but what magic amateursminded\". He raises hands out towards another audience table across London smoking tin cans filled exceptwith potion boxes etc.--\\nThe door opens revealing Lord Albinus Blackheart drinking wine. One bottle contains twelve pints(s.)\\'in four pieces.\" He puts himself between Percy Fleurius \\'brother\\'. It appears black men always wear red hair during Halloween night unless someone leaves home early and frighten relatives. On horseback running straight off headways, William Cowper enters holding syringes wrapped inside two small scissors cut ends pointing toward Hushpuff Fudge. Suddenly silence begins throughout Great Hall : quiet enough under normal circumstances for people else reading lines at schoolchildren alike before beginning gesticulating wildly (from each side). Then Bill Burrow stares blank expressionfully at Miss Granger walking alongside late summer day girls wearing yellow stumps being carried out neatly around the wall \"CUT TO ONCE ADMIRAL PELTORUS TURNS HIS WESTERN CIRCUMSTANCE AS THE BABY IS SLOW DOWN...\"\\nProfessor Maclennan arrives flanked first upon right winged wizard students pulling torches ready every thirty six hours dressed suitcases crammed full together waiting frantically alone amongst police line vans... leaving both Dumbledore & Co surrounded nearby screaming \"YOU WANT ME KILLED!\" Behind Terry Allen trying desperately help getting Harry stuck within twenty minutes carrying George Fox Jr, and now Dean Devlin assisting Dr Evans McArthur on arriving on Dursleys doorstep (when Sir Douglas Booth rushes and picks him dead - Voldemort gets shot in arm!)... McGonagall seems delighted indeed..  Or maybe this thing happened because a sick little mouse had wandered past the wards: Snape wouldnt bey once seen coming past. At Wizengamot everyone sits relaxed except McGoneyshete Potions instructor Dudley Morgan studying Professor Watson without even bothering stopping till the whole street blaring music \"...you won\\'t mind thinking?\" Everyone nods happily oblivious also after eating/drinking a lot sleep pills for weeks long battle preparations ahead showing Neville still alert knowing when things start happening correctly and the Death Eaters preparing everything... well those lucky ones are still going crazy!!\\n\\nDraymond Bellastrobus tries moving Draco Malfoy down toilet bowl after shower stall with broomstick. This brings up Tom Percival Snowwhite turning away empty seat next stirrups flicker on monitor in excitement... Fluttershy turns back around hesitating. Finally Sirius sets sail with great gusto and drifts forward swinging batonically over bedside windows displaying impressive pupils including eleven... Minerva Stone gazing intently beyond Headlocker hoping Severus makes news when Slughorn interrupts him crying softly...\\nSirius watches with fascination as Lily stands there staring terrified. As Severus catches sight Vernon pondering the consequences Of Being A Gryffindor Who Can Only Be Stopped All Too Early by Aunt Petunia Soarin having gone AWOL AGAINST THEM Inappropriately Now That They Just Thinking intelligent\\xa0  Whispering about Horace Munster giving Dad a rambling talk, it might seem rather dandy... But carefully chosen Harry managed simply remain silent... Hedwig Scrotum looks at Hagrid leaning back on tattered blankets facing Leliana Bathory wondering what she was doing...Hermione stares confused but worried as Lupin squeals slightly. After Draco slaps Narcissa Malfoy into'}]\n"
          ]
        }
      ],
      "source": [
        "input_prompt = \"<BOS><Thriller><J. K. Rowling>\"\n",
        "story = story_generator(input_prompt, max_length=1024, do_sample=True,\n",
        "               repetition_penalty=1.1, temperature=1.2, \n",
        "               top_p=0.95, top_k=50)\n",
        "print(story)\n",
        "f = open(\"story.html\", \"w+\")\n",
        "f.write(story[0]['generated_text'])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/story_generator_checkpoint.zip /content/story_generator_checkpoint\n",
        "from google.colab import files\n",
        "files.download(\"/content/story_generator_checkpoint.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "nZ7vOkG_Jb3c",
        "outputId": "0f252716-1e4c-41e5-ff30-3dde9009e205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/story_generator_checkpoint/ (stored 0%)\n",
            "  adding: content/story_generator_checkpoint/checkpoint-1000/ (stored 0%)\n",
            "  adding: content/story_generator_checkpoint/checkpoint-1000/trainer_state.json (deflated 52%)\n",
            "  adding: content/story_generator_checkpoint/checkpoint-1000/pytorch_model.bin"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BuildMoviePlotGenerator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}